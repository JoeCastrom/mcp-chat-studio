# ğŸš€ MCP Chat Studio

**The Ultimate Testing Platform for MCP Servers**

Test, debug, and develop Model Context Protocol servers with a beautiful glassmorphism UI.  
**Record scenarios** â€¢ **Compare LLM responses** â€¢ **Generate mock servers** â€¢ **Zero config with Ollama**

![MCP Chat Studio](https://img.shields.io/badge/MCP-Chat%20Studio-blueviolet?style=for-the-badge)
![Node.js](https://img.shields.io/badge/Node.js-18+-green?style=flat-square)
![License](https://img.shields.io/badge/license-MIT-blue?style=flat-square)
[![CI](https://github.com/JoeCastrom/mcp-chat-studio/actions/workflows/ci.yml/badge.svg)](https://github.com/JoeCastrom/mcp-chat-studio/actions/workflows/ci.yml)
![Docker](https://img.shields.io/badge/docker-ready-blue?style=flat-square&logo=docker)

![Demo](docs/gif/showcase.gif)

> **ğŸ“Œ Local MCP Test Bench** - Designed for development. Not intended for internet exposure.

---

## âš¡ Get Started in 60 Seconds

```bash
# 1. Clone and install
git clone https://github.com/JoeCastrom/mcp-chat-studio.git && cd mcp-chat-studio && npm install

# 2. Start (works with Ollama out of the box - no API keys needed!)
npm run dev

# 3. Open http://localhost:3082 and start testing! ğŸ‰
```

**That's it!** You now have a complete MCP testing platform running locally.

---

## ğŸ†š Why Not Just Use Claude Desktop or ChatGPT?

| Feature                          | MCP Chat Studio |  Claude Desktop  |     ChatGPT      |
| -------------------------------- | :-------------: | :--------------: | :--------------: |
| Test ALL tools automatically     |       âœ…        |        âŒ        |        âŒ        |
| Record & Replay scenarios        |       âœ…        |        âŒ        |        âŒ        |
| Response diffing                 |       âœ…        |        âŒ        |        âŒ        |
| Custom assertions (14 operators) |       âœ…        |        âŒ        |        âŒ        |
| Low-level MCP debugging          |       âœ…        |        âŒ        |        âŒ        |
| Mock server generator            |       âœ…        |        âŒ        |        âŒ        |
| 8 LLM providers                  |       âœ…        | âŒ (Claude only) | âŒ (OpenAI only) |
| Local/no API keys needed         |   âœ… (Ollama)   |        âŒ        |        âŒ        |
| Multi-environment profiles       |       âœ…        |        âŒ        |        âŒ        |
| Session branching                |       âœ…        |        âŒ        |        âŒ        |

---

## ğŸ¨ NEW: Visual MCP Server Generator

Create production-ready MCP servers without writing boilerplate code!

- ğŸ¨ **Visual tool designer** with parameters and types
- ğŸ **Python (mcp SDK)** and ğŸ“¦ **Node.js (TypeScript)** code generation
- âš¡ Copy to clipboard or download instantly
- ğŸ”§ Perfect for prototyping and teaching MCP

---

## ğŸ¯ Perfect For

- **ğŸ”§ MCP Server Developers** - Test your tools without manual clicking
- **ğŸ¤– AI App Builders** - Compare GPT-4 vs Claude vs Llama on the same tasks
- **ğŸ¢ Enterprise Teams** - Share test scenarios and environments
- **ğŸ› Debugging** - See raw MCP protocol messages when things break
- **ğŸ“š Learning MCP** - Visual tool designer teaches protocol structure

---

## âœ¨ Why MCP Chat Studio?

- **ğŸ¯ Built for MCP Development** - Test and debug MCP servers without writing code
- **ğŸ”§ 8 LLM Providers** - Ollama, OpenAI, Claude, Gemini, Azure, Groq, Together AI, OpenRouter
- **ğŸ§ª Test Scenarios** - Record, replay, and validate tool executions
- **ğŸ“Š Response Diffing** - Semantic JSON comparison with color-coded changes
- **ğŸ“‹ Schema Validation** - Contract testing with auto-inferred schemas
- **ğŸ” Custom Assertions** - 14 operators with JSONPath support
- **ğŸ” Low-Level Debugging** - Inspector tab for raw MCP protocol inspection
- **ğŸ³ Production Ready** - Docker support, CI/CD, security-hardened
- **ğŸ’¡ Zero Config Start** - Works with Ollama out of the box, no API keys needed
- **ğŸ¨ Beautiful UI** - Modern glassmorphism design with dark/light themes

---

## ğŸ“¸ Screenshots

<details>
<summary>Click to view more screenshots</summary>

### Light Mode

![Light Mode](docs/images/main-page-light-mode.png)

### Adding MCP Servers

![Add Server](docs/images/add-mcp-page.png)

### Tool Inspector (Low-Level Debugging)

![Inspector](docs/images/inspector-page.png)

### Tool Testing

![Test All Tools](docs/images/test-all-tools.png)

### Keyboard Shortcuts

![Shortcuts](docs/images/keyboard-shortcuts.png)

</details>

---

## ğŸš€ Features

### ğŸ’¬ Multi-Provider LLM Chat

- **Ollama** - Local LLM (llama3, mistral, qwen, etc.)
- **OpenAI** - GPT-4o, GPT-4, GPT-3.5
- **Anthropic** - Claude 3.5, Claude 3
- **Google Gemini** - Gemini Pro, Gemini Flash
- **Azure OpenAI** - Enterprise Azure deployments
- **Groq** - Ultra-fast inference (Mixtral, LLaMA)
- **Together AI** - Open-source models
- **Real-time streaming** with typing effect

### ğŸ”§ MCP Tool Management

- **Dynamic server management** - Add/remove servers at runtime
- **STDIO & SSE transports** supported
- **Environment variables** - Configure API keys, URLs per server
- **Import YAML/JSON** - Paste config from docs
- **Config preview** - See generated config before adding

### ğŸ§ª Tool Testing

- **Test All Tools** - Smoke test all connected tools
- **Response preview** - See what each tool returns
- **Timing** - Measure tool response times
- **Safe mode** - Skip risky tools (Click, Type, Launch)
- **Error detection** - Uses MCP's `isError` field

### ğŸ”§ Inspector Tab (Low-Level Debugging)

- **Manual tool execution** - Call any tool with custom arguments
- **Auto-generated forms** - Input fields from JSON schema
- **Protocol log** - See raw MCP request/response JSON
- **SSE event viewer** - Real-time server events for SSE transports
- **Resources/Prompts API** - Full MCP protocol support

### ğŸ“¤ Config Export/Import

- **Export** - Download your config as YAML
- **Import** - Load config from YAML/JSON file
- **Team sharing** - Share configs across machines

### ğŸ§ª Test Scenarios (Record/Replay)

- **Recording** - Click "ğŸ”´ Start Recording" to capture tool executions
- **Step capture** - Records tool name, args, response, timing, schema
- **Save scenarios** - Name and save as JSON to localStorage
- **Replay** - Run all steps with âœ…/âŒ/ğŸ”¶ pass/fail status
- **Export** - Download scenarios as JSON for Git/CI integration

### ğŸ“Š Response Diffing

- **Semantic comparison** - Not raw text diff, but JSON-aware
- **Color-coded** - ğŸ”´ missing, ğŸŸ¢ added, ğŸŸ¡ changed, ğŸŸ  type change
- **Side-by-side view** - Modal shows baseline vs current
- **Breaking change detection** - Flags structural changes

### ğŸ“‹ Schema Validation

- **Auto-inference** - Generates schema from first "good" response
- **Contract testing** - Validates responses against saved schema
- **Inline results** - Shows "ğŸ“‹ Schema OK" or "ğŸ“‹ N issues"
- **Violation details** - Missing fields, type mismatches, extra fields

### ğŸ”’ Flexible Authentication

- **Keycloak** - Full OIDC with PKCE
- **GitHub** - OAuth2 preset
- **Google** - OAuth2 preset
- **Custom** - Any OAuth2 provider with custom URLs

### ğŸ¨ Modern UI

- **Glassmorphism design** - Frosted glass aesthetic
- **Dark/Light theme** - Toggle with ğŸŒ™/â˜€ï¸
- **Tool schema viewer** - See parameter details inline
- **Responsive layout** - Works on all screen sizes

### âŒ¨ï¸ Keyboard Shortcuts

| Shortcut       | Action               |
| -------------- | -------------------- |
| `Enter`        | Send message         |
| `Shift+Enter`  | New line             |
| `Escape`       | Cancel / Close modal |
| `Ctrl+K`       | Focus tool search    |
| `Ctrl+Shift+E` | Export chat          |
| `Ctrl+/`       | Show shortcuts help  |

### ğŸ“Š Token Usage Display

- **Real-time tracking** - Input/output tokens per session
- **Cost estimation** - Supports 8 LLM providers
- **Header badge** - Click for detailed breakdown
- **Reset option** - Start fresh anytime

### ğŸ­ System Prompt Library

- **5 Presets** - Default, Strict Coder, JSON Validator, Creative Writer, Tool Tester
- **Custom prompts** - Create and save your own personas
- **Quick switch** - Dropdown in chat panel
- **Prompt Manager** - Edit/delete saved prompts

### ğŸ“¦ Test Suites

- **Group scenarios** - Combine related tests
- **Batch execution** - Run entire suite at once
- **Aggregate results** - Pass/fail summary
- **Last run tracking** - Timestamp and stats

### ğŸ” Custom Assertions

- **14 operators** - equals, contains, matches, exists, type, length, gt, lt, gte, lte, etc.
- **JSONPath support** - Deep value access with `$.path.to.value`
- **Flexible validation** - Beyond simple equality checks

### ğŸ’» CLI Test Runner

Run scenarios from command line for CI/CD integration:

```bash
# Run tests
mcp-cli test scenarios.json

# JUnit output for CI
mcp-cli test scenarios.json --output junit

# Fail on any diff
mcp-cli test scenarios.json --fail-on-diff

# Custom server URL
mcp-cli test scenarios.json --server http://staging:3082
```

### âš™ï¸ Mock MCP Server Generator

- **Visual designer** - Create tools with parameters
- **Code generation** - Python (mcp SDK) or Node.js (TypeScript)
- **Copy/Download** - Get starter code instantly
- **Parameter types** - string, number, boolean, object

### ğŸŒ¿ Session Branching

- **Fork conversations** - Branch at any message
- **Save snapshots** - Preserve conversation states
- **Load branches** - Continue from any saved point
- **Branch manager** - View, load, delete branches

### ğŸŒ Multi-Environment Profiles

- **Dev/Staging/Prod** - Switch between environments
- **Auto-save config** - Per-environment settings
- **Quick switching** - Sidebar dropdown

---

## ğŸ“¦ Installation

### Prerequisites

- **Node.js** 18+
- **npm** or **yarn**

### Quick Start

```bash
# Clone the repository
git clone https://github.com/JoeCastrom/mcp-chat-studio.git
cd mcp-chat-studio

# Install dependencies
npm install

# Copy environment template
cp .env.example .env

# Start the server
npm run dev
```

Open **http://localhost:3082** in your browser.

---

## ğŸ³ Docker Deployment

### Using Docker Compose (Recommended)

#### Option 1: Use Your Local Ollama (Default)

If you already have Ollama installed on your machine:

```bash
# Start MCP Chat Studio only (uses your local Ollama)
docker-compose up

# Run in background
docker-compose up -d
```

The app automatically connects to your local Ollama at `http://localhost:11434`.

#### Option 2: Include Ollama in Docker

If you don't have Ollama installed locally:

```bash
# Start both MCP Chat Studio AND Ollama in Docker
docker-compose --profile with-ollama up

# Run in background
docker-compose --profile with-ollama up -d
```

**Services started:**

- MCP Chat Studio: http://localhost:3082
- Ollama (if using profile): http://localhost:11434

#### Configure API Keys

```bash
# Create .env file or set environment variables
OPENAI_API_KEY=sk-your-key
ANTHROPIC_API_KEY=sk-ant-your-key
GOOGLE_API_KEY=your-google-key
```

### Using Docker Only

```bash
# Build image
docker build -t mcp-chat-studio .

# Run with Ollama (default)
docker run -p 3082:3082 mcp-chat-studio

# Run with OpenAI
docker run -p 3082:3082 \
  -e OPENAI_API_KEY=sk-your-key \
  mcp-chat-studio

# Run with custom config
docker run -p 3082:3082 \
  -v $(pwd)/config.yaml:/app/config.yaml:ro \
  mcp-chat-studio
```

### Health Check

```bash
curl http://localhost:3082/api/health
```

**Response:**

```json
{
  "status": "ok",
  "mcpServers": {
    "server-name": "connected"
  }
}
```

---

## âš™ï¸ Configuration

### LLM Providers

MCP Chat Studio supports **8 LLM providers**. Configure in `config.yaml`:

#### Ollama (Local - Default)

```yaml
llm:
  provider: ollama
  model: llama3.2
```

No API key needed. Just run `ollama serve`.

#### OpenAI

```yaml
llm:
  provider: openai
  model: gpt-4o
```

```bash
# .env
OPENAI_API_KEY=sk-your-key
```

#### Anthropic Claude

```yaml
llm:
  provider: anthropic
  model: claude-3-5-sonnet-20241022
```

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-your-key
```

#### Google Gemini

```yaml
llm:
  provider: gemini
  model: gemini-1.5-flash
```

```bash
# .env
GOOGLE_API_KEY=your-google-ai-key
```

#### Azure OpenAI

```yaml
llm:
  provider: azure
  model: gpt-4o
```

```bash
# .env
AZURE_OPENAI_API_KEY=your-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_DEPLOYMENT=your-deployment-name
```

#### Groq (Ultra-fast)

```yaml
llm:
  provider: groq
  model: llama-3.3-70b-versatile
```

```bash
# .env
GROQ_API_KEY=gsk_your-key
```

#### Together AI

```yaml
llm:
  provider: together
  model: meta-llama/Llama-3.3-70B-Instruct-Turbo
```

```bash
# .env
TOGETHER_API_KEY=your-key
```

#### OpenRouter (100+ Models)

Access Claude, GPT-4, Gemini, Llama, and 100+ more models through a single API:

```yaml
llm:
  provider: openrouter
  model: anthropic/claude-3.5-sonnet # or openai/gpt-4o, google/gemini-pro-1.5
```

```bash
# .env
OPENROUTER_API_KEY=your-key
# Get your key at https://openrouter.ai/keys
```

### MCP Servers (`config.yaml`)

```yaml
mcpServers:
  my-mcp-server:
    type: stdio
    command: python
    args:
      - -m
      - my_mcp_server
    env:
      API_KEY: '${API_KEY}' # From .env
    description: 'My custom MCP server'
    startup: true # Auto-connect on startup
```

---

## ğŸ› ï¸ Adding MCP Servers

### Via UI (Recommended)

1. Click **+ Add** in the sidebar
2. Fill in server details:
   - **Name**: Unique identifier
   - **Transport**: STDIO or SSE
   - **Command/URL**: How to start/connect
   - **Arguments**: Command-line args
   - **Environment Variables**: API keys, URLs
3. Preview the generated config
4. Click **Add Server**

### Via Import

1. Click **ğŸ“‹ Import YAML/JSON**
2. Paste your config:

```yaml
command: python
args:
  - -m
  - my_server
env:
  API_KEY: sk-xxx
```

3. Click **Import** â†’ Form auto-fills
4. Review and **Add Server**

---

## ğŸ¯ Example MCP Servers

Get started quickly with these popular MCP servers:

### GitHub MCP Server

Access GitHub repositories, issues, PRs, and more:

```yaml
mcpServers:
  github:
    type: stdio
    command: npx
    args:
      - '@modelcontextprotocol/server-github'
    env:
      GITHUB_TOKEN: '${GITHUB_TOKEN}'
    description: 'GitHub API integration'
    startup: true
```

**In .env:**

```bash
GITHUB_TOKEN=ghp_your_github_personal_access_token
```

**Installation:**

```bash
npm install -g @modelcontextprotocol/server-github
```

### Filesystem MCP Server

Secure file operations with access control:

```yaml
mcpServers:
  filesystem:
    type: stdio
    command: npx
    args:
      - '@modelcontextprotocol/server-filesystem'
      - '/path/to/allowed/directory'
    description: 'File system operations'
```

**Note:** Only has access to the specified directory for security.

### Brave Search MCP Server

Web search capabilities:

```yaml
mcpServers:
  brave-search:
    type: stdio
    command: npx
    args:
      - '@modelcontextprotocol/server-brave-search'
    env:
      BRAVE_API_KEY: '${BRAVE_API_KEY}'
    description: 'Web search via Brave'
```

### Puppeteer MCP Server

Browser automation and web scraping:

```yaml
mcpServers:
  puppeteer:
    type: stdio
    command: npx
    args:
      - '@modelcontextprotocol/server-puppeteer'
    description: 'Browser automation'
```

### More Official Servers

- **@modelcontextprotocol/server-postgres** - PostgreSQL database access
- **@modelcontextprotocol/server-sqlite** - SQLite database operations
- **@modelcontextprotocol/server-slack** - Slack integration
- **@modelcontextprotocol/server-google-maps** - Google Maps API

**Find more:** [MCP Servers Directory](https://github.com/modelcontextprotocol/servers)

---

## ğŸ§ª Testing Tools

### Test All Tools

1. Connect to an MCP server
2. Click **ğŸ§ª Test All Tools**
3. View results:
   - âœ… Tool works with minimal input
   - âš ï¸ Tool responded but input validation failed
   - âŒ Tool failed completely

### Risky Tools

Some tools have side effects and are **skipped by default**:

- `Click-Tool` - Clicks on screen
- `Type-Tool` - Types text
- `Launch-Tool` - Opens applications
- `Drag-Tool`, `Key-Tool`, `Shortcut-Tool`, `Scroll-Tool`

Check **âš ï¸ Include risky tools** to test them.

### Response Data

- Each test shows a **preview** of the response
- Click **ğŸ“‹ Copy** to copy full JSON
- **Duration** shows response time in ms

---

## ğŸ§ª Test Scenarios (Advanced Testing)

Test Scenarios let you **record, replay, and validate** tool executions for regression testing.

### Workflow

```
1. RECORD â†’ Execute tools normally, they're saved as baseline
2. REPLAY â†’ Re-run all steps, compare responses
3. ANALYZE â†’ See diffs and schema violations
```

### Recording a Scenario

1. Go to **ğŸ§ª Scenarios** tab
2. Click **ğŸ”´ Start Recording**
3. Switch to **ğŸ”§ Inspector** tab
4. Execute your tools (each becomes a step)
5. Return to **ğŸ§ª Scenarios** tab
6. Click **â¹ï¸ Stop Recording**
7. Name and **ğŸ’¾ Save** your scenario

Each step captures:

- Tool name & server
- Arguments used
- Response (as baseline)
- Response hash (for quick comparison)
- Inferred schema (for contract testing)
- Execution timing

### Replaying a Scenario

1. Find your scenario in **Saved Scenarios**
2. Click **â–¶ï¸ Replay**
3. Watch results appear:
   - âœ… **Pass** - Response matches baseline
   - ğŸ”¶ **Diff** - Response differs (click "View Diff")
   - âŒ **Fail** - Execution error
   - ğŸ“‹ **Schema** - Validation result

### Response Diffing

When a step shows ğŸ”¶, click **View Diff** to see:

| Color          | Meaning                            |
| -------------- | ---------------------------------- |
| ğŸ”´ **MISSING** | Field was in baseline, now gone    |
| ğŸŸ¢ **ADDED**   | New field appeared                 |
| ğŸŸ¡ **CHANGED** | Same key, different value          |
| ğŸŸ  **TYPE**    | Type changed (e.g., stringâ†’number) |

### Schema Validation

Each step's response schema is **auto-inferred** during recording. On replay:

- **Type checks** - Expected `string`, got `number`
- **Required fields** - Field was present, now missing
- **Extra fields** - Unexpected new fields (warning)

Results show as **ğŸ“‹ Schema OK** or **ğŸ“‹ N issues**.

### Exporting Scenarios

- **Single**: Click **ğŸ“¦ Export** on a scenario
- **All**: Click **ğŸ“¦ Export All**
- Format: JSON (version-controllable in Git)

## ğŸ’¬ Using the Chat

### Basic Chat

1. Type a message and press Enter
2. The LLM responds (with streaming if enabled)

### Using MCP Tools

1. Enable **Use MCP Tools** checkbox
2. The LLM can call any connected tool
3. Tool results appear in the chat

### Streaming

- Enable **âš¡ Stream** for real-time typing effect
- Streaming is disabled when tools are enabled (tools need full response)

### Force Tool Mode

1. Click a tool in the sidebar
2. Click **Force** to require the LLM to use it
3. Badge shows which tool is forced

---

## ğŸ”§ Inspector Tab

The Inspector provides **low-level MCP debugging** without using the LLM.

### How to Use

1. Click the **ğŸ”§ Inspector** tab
2. Select a **Server** from the dropdown
3. Select a **Tool** from the dropdown
4. Fill in the **parameters** (auto-generated from schema)
5. Click **â–¶ï¸ Execute**
6. View the **raw JSON response**

### Input Types

| Schema Type | Input Field           |
| ----------- | --------------------- |
| string      | Text input            |
| number      | Number input          |
| boolean     | True/False dropdown   |
| array       | JSON textarea         |
| object      | JSON textarea         |
| enum        | Dropdown with options |

### When to Use

- **Debugging** - Test tools without LLM interpretation
- **Development** - Iterate on tool parameters quickly
- **Verification** - Check exact MCP responses

---

## ğŸ“ Project Structure

```
mcp-chat-studio/
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html        # Single-page UI (HTML + CSS + JS)
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ index.js          # Express server entry
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ chat.js       # Chat & LLM endpoints
â”‚   â”‚   â”œâ”€â”€ llm.js        # LLM settings endpoints
â”‚   â”‚   â”œâ”€â”€ mcp.js        # MCP management endpoints
â”‚   â”‚   â””â”€â”€ oauth.js      # OAuth endpoints
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ LLMClient.js  # Multi-provider LLM client
â”‚       â””â”€â”€ MCPManager.js # MCP server manager
â”œâ”€â”€ config.yaml           # MCP server configs
â”œâ”€â”€ .env                  # Environment variables
â””â”€â”€ package.json
```

---

## ğŸ”Œ API Endpoints

### Chat

| Method | Endpoint             | Description                        |
| ------ | -------------------- | ---------------------------------- |
| POST   | `/api/chat`          | Send message (streaming supported) |
| POST   | `/api/chat/continue` | Continue after tool call           |

### MCP

| Method | Endpoint                   | Description             |
| ------ | -------------------------- | ----------------------- |
| GET    | `/api/mcp/status`          | Get server statuses     |
| GET    | `/api/mcp/tools`           | Get all available tools |
| POST   | `/api/mcp/add`             | Add new server          |
| POST   | `/api/mcp/connect`         | Connect to server       |
| POST   | `/api/mcp/disconnect`      | Disconnect from server  |
| POST   | `/api/mcp/call`            | Call a tool             |
| DELETE | `/api/mcp/remove/:name`    | Remove server           |
| GET    | `/api/mcp/resources/:name` | List resources          |
| POST   | `/api/mcp/resources/read`  | Read a resource         |
| GET    | `/api/mcp/prompts/:name`   | List prompts            |
| POST   | `/api/mcp/prompts/get`     | Get a prompt            |

### LLM

| Method | Endpoint            | Description              |
| ------ | ------------------- | ------------------------ |
| GET    | `/api/llm/settings` | Get current LLM settings |
| POST   | `/api/llm/settings` | Update LLM settings      |

---

## ğŸ¨ Theming

Toggle between **Dark** and **Light** mode using the ğŸŒ™/â˜€ï¸ button in the header.

Theme is saved to localStorage and persists across sessions.

---

## ğŸ”’ Authentication (Optional)

MCP Chat Studio supports multiple OAuth2 providers:

### Provider Presets

```yaml
oauth:
  provider: keycloak # or github, google
  client_id: '${OAUTH_CLIENT_ID}'
  client_secret: '${OAUTH_CLIENT_SECRET}'
  redirect_uri: 'http://localhost:3082/api/oauth/callback'

  # Keycloak-specific
  keycloak_url: 'https://your-keycloak/auth'
  keycloak_realm: 'your-realm'
```

### Custom OAuth2 Provider

```yaml
oauth:
  authorize_url: 'https://provider.com/oauth/authorize'
  token_url: 'https://provider.com/oauth/token'
  userinfo_url: 'https://provider.com/api/userinfo'
  client_id: '${OAUTH_CLIENT_ID}'
  client_secret: '${OAUTH_CLIENT_SECRET}'
  scopes: ['openid', 'profile', 'email']
  use_pkce: true # false for legacy providers
```

### Environment Variables

```bash
OAUTH_CLIENT_ID=your-client-id
OAUTH_CLIENT_SECRET=your-secret
OAUTH_AUTHORIZE_URL=https://...  # For custom providers
OAUTH_TOKEN_URL=https://...
```

Click **Login** to authenticate.

---

## ğŸ” Security Best Practices

### Important Security Notes

- âš ï¸ **Never commit `.env` file** - Contains API keys and secrets
- âš ï¸ **Use environment variables** - All secrets should be in `.env` or environment
- âš ï¸ **SSL verification enabled** - Disabled only for dev with self-signed certs
- âš ï¸ **OAuth tokens in memory** - Production: use Redis for persistence

### For Development

```bash
# .env (local development)
OPENAI_API_KEY=sk-your-key
ANTHROPIC_API_KEY=sk-ant-your-key

# Disable SSL verify only for internal/self-signed certs
OAUTH_DISABLE_SSL_VERIFY=true
```

### For Production

```bash
# Use secure environment variables
export OPENAI_API_KEY="sk-your-key"
export ANTHROPIC_API_KEY="sk-ant-your-key"

# Never disable SSL verification in production
# OAUTH_DISABLE_SSL_VERIFY should NOT be set

# Use Redis for OAuth tokens (multi-instance deployments)
# Modify server/services/OAuthManager.js to use Redis instead of Map
```

### MCP Server Security

- **Filesystem Server:** Only grant access to specific directories
- **Database Servers:** Use read-only credentials when possible
- **API Keys:** Store in `.env`, never in `config.yaml`
- **Network Servers (SSE):** Use HTTPS and authentication

### What's Safe to Commit

âœ… **Safe:**

- `config.yaml.example`
- `.env.example`
- Source code
- Documentation

âŒ **NEVER Commit:**

- `.env` (actual credentials)
- `config.yaml` (may contain secrets)
- API keys or tokens
- Certificates or private keys

---

## â“ Frequently Asked Questions

### General

**Q: Can I use multiple MCP servers at once?**
A: Yes! Add as many as you want. Tools are automatically namespaced (e.g., `github__get_issue`, `filesystem__read_file`) to prevent conflicts.

**Q: Do I need API keys to start?**
A: No! Use Ollama locally (free, no keys required). API keys are only needed for cloud LLM providers like OpenAI, Anthropic, etc.

**Q: Can I use my own MCP server?**
A: Absolutely! Add it via the UI or edit `config.yaml`. Supports both STDIO and SSE transports.

**Q: Does this work on Windows?**
A: Yes! Works on Windows, macOS, and Linux. For Windows, use PowerShell or Git Bash for commands.

### Usage

**Q: How do I test tools without the chat?**
A: Use the **Inspector** tab to call tools directly without involving the LLM. Perfect for debugging and testing.

**Q: Why can't I use streaming with tools?**
A: LLMs need the complete response to decide which tools to call. Streaming is auto-disabled when tools are enabled.

**Q: What are "risky tools"?**
A: Tools that have side effects (clicking, typing, launching apps). They're skipped by default in "Test All Tools" to prevent unwanted actions.

**Q: How do I switch LLM providers?**
A: Click **âš™ï¸ Settings** â†’ Change provider and model â†’ Save. Changes take effect immediately.

### MCP Servers

**Q: What's the difference between STDIO and SSE?**
A: **STDIO** runs locally as a subprocess. **SSE** connects to remote servers over HTTP. Use STDIO for local tools, SSE for network services.

**Q: Can MCP servers see my API keys?**
A: Only if you explicitly pass them via environment variables. Each server only sees the env vars you configure for it.

**Q: My MCP server won't connect. What's wrong?**
A: Check:

1. Command/path is correct
2. Required dependencies installed (`npm install -g ...`)
3. Environment variables set
4. Server logs in browser console (F12)

**Q: Can I use the same MCP server with different configurations?**
A: Yes! Add it multiple times with different names and configs. For example, `github-personal` and `github-work` with different tokens.

### Development

**Q: How do I add a new LLM provider?**
A: Edit `server/services/LLMClient.js`. See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

**Q: Can I contribute new features?**
A: Yes! We welcome contributions. See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Q: Is there an API for automation?**
A: Yes! See the **API Endpoints** section. All features are accessible via REST API.

---

## ğŸ› Troubleshooting

### MCP Server Won't Connect

1. Check the command/path is correct
2. Verify environment variables
3. Check server logs in console
4. Try running the command manually

### LLM Not Responding

1. Check provider settings in âš™ï¸ Settings
2. Verify Ollama is running (`ollama serve`)
3. Check API key for OpenAI

### Tools Show âš ï¸ Warning

This means the tool **responded** but our dummy test input was invalid. The tool is working - it just needs proper arguments.

---

## ğŸ“œ License

MIT License - feel free to use and modify.

---

## ğŸ¤ Contributing

We welcome contributions from the community! Whether it's bug fixes, new features, documentation improvements, or examples - all contributions are appreciated.

### How to Contribute

1. **Fork** the repository
2. **Clone** your fork: `git clone https://github.com/YOUR_USERNAME/mcp-chat-studio.git`
3. **Create a branch**: `git checkout -b feature/amazing-feature`
4. **Make changes** and test thoroughly
5. **Format code**: `npm run format`
6. **Lint code**: `npm run lint`
7. **Commit**: `git commit -m 'feat: add amazing feature'`
8. **Push**: `git push origin feature/amazing-feature`
9. **Open a Pull Request**

### Contribution Guidelines

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for:

- Code style guidelines
- Commit message conventions
- Testing requirements
- Pull request process

### Found a Bug?

[Open an issue](https://github.com/JoeCastrom/mcp-chat-studio/issues/new?template=bug_report.md) with:

- Clear description
- Steps to reproduce
- Expected vs actual behavior
- Screenshots if applicable
- Environment details (OS, Node version, etc.)

### Want a Feature?

[Request it here](https://github.com/JoeCastrom/mcp-chat-studio/issues/new?template=feature_request.md) with:

- Use case description
- Proposed solution
- Alternatives considered

### Quick Contribution Ideas

- ğŸ› Fix a bug from [issues](https://github.com/JoeCastrom/mcp-chat-studio/issues)
- ğŸ“š Improve documentation
- ğŸ§ª Add tests
- ğŸ¨ Improve UI/UX
- ğŸ”Œ Add new LLM provider
- ğŸ“¦ Create example MCP servers
- ğŸŒ Add translations

---

## ğŸ™ Acknowledgments

- **Created by** [Youssef Ghazi](https://github.com/JoeCastrom)
- Built with [Model Context Protocol](https://modelcontextprotocol.io)
- Inspired by the MCP community
- Thanks to all contributors!

---

**Built with â¤ï¸ by Youssef Ghazi for MCP developers**
